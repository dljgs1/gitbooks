## 摘要
机器学习已经成为日用品。大量的机器学习框架和服务对于并非机器学习专家的数据持有者变得容易获取。当机器学习模型对于敏感输入（比如个人图片和文档）进行训练时，不要泄露太多信息是很重要的。

我们先考虑一个恶意地给用户提供机器学习训练代码的提供者，不观察训练，而是获取对结果模型黑盒或白盒的访问。在这个设定下，我们设计和实现了一些实际的算法，其中一些和标准的机器学习技术比如正则化和数据膨胀（data augmentation）很相似的，能够“记忆”训练集模型的信息，这个模型和传统的训练模型达到同样的精确度和可预测度。我们解释了我们的对手是如何能够从模型中撺取记忆信息的。

我们在通过在标准机器学习上进行图像分类（CIFAR10）、人脸识别（LFW and FaceScrub）和文本分析（20 Newsgroups and IMDB）的任务，对我们的模型进行评估。在所有的案例中，我们演示我们的算法如何创造这样一个具有高预测能力同时也允许对训练数据的精确萃取的模型。

## CCS概念
- 安全性和隐私 -> 软件和应用安全

## 关键词
隐私，机器学习

## 1 导语
机器学习（ML）成功地应用在许多数据分析任务中，从图像识别到零售购物预测。大量的ML库的服务在网上都很容易获得（见2.2），而且每年都有新的出现。

数据持有者想应用ML技术到它们的数据集上，这之中许多都包括敏感数据，而且很多使用者可能不是机器学习专家。他们把第三方的ML代码拿来用却不知道代码具体在做什么。只要结果模型的预测能力高，他们就不会问“这个模型对我的训练数据还获取了什么”。

现代机器学习模型，特别是人工神经网络，对记忆任何信息都有巨大的容量。这回导致过量供应：即使是一个精确模型都只需要用一小部分。ML库的提供者或者ML服务的操作可以修改训练算法是的模型能够解码出更多关于训练集的信息，同时还保证主要任务的高精度。

我们的贡献：我们演示了对训练算法较小的修改可以产生拥有和标准ML度量（比如精确性和泛化能力）下的高精度模型，这种模型可以泄露训练集的细节信息。

我们假设恶意的ML提供者供应给数据持有者训练算法，而数据持有者没有发现它的执行。在模型创建后，提供者获得了整个模型（白盒子）或者获得其输入输出端口（黑盒子）。提供者此时意图将训练集信息从模型中分离出来。在数据持有者使用恶意第三方ML库以及市面上的算法时会出现，这使得数据持有者向第三方支付数据环境安全的费用。

在白盒子案例中，我们评估了几个技术：1）将敏感信息编码到模型参数中最不重要的部分2）迫使参数与敏感信息高度相关；3）在参数符号中编码敏感信息。后两个技术包括添加恶意的“正则化”项目到损失函数中，而在数据持有者角度看上去像另一个正则化技术。

在黑盒子案例中，我们使用了一个类似数据膨胀（用额外生成的数据拓展数据集）的技术而且不对训练算法做任何修改。结果模型因此训练了两份任务。在一开始，是数据持有者指定的分类任务。恶意任务紧随其后：给一个特定的生成输入，“预测”一个或者更多实际训练集的秘密的信息。

因为与我们的合成增广输入相关的标签编码了有关训练数据的秘密，它们不符合这些输入中的任何结构。因此，我们的次要任务要求模型“学习”基本是随机标注的东西。然而，我们从经验上证明，模型过拟合了合成的输入，而且不影响它们对主要任务的准确性和泛化性。这使得黑盒子信息的以抽离，比如：对方提供了一个合成的输出，模型输出其标签，此时实际训练集的秘密信息在训练过程中被记忆。

![](pics\fig1.png)

我们在几个标准数据集上评估了黑白盒的恶意训练技术：CIFAR10（图像分类）Labeled Face和Wild（人脸识别），FaceScrub（性别分类和人脸识别）以及IMDB（二元情绪分类）。在所有的案例中，恶意训练模型表现出和传统训练模型相似的精度和泛化能力。

我们证明了恶意方是如何将子集从训练模型中抽取出来的，以及参数的选择是如何影响抽取的数量和精度的。比如，白盒攻击直接从模型参数上解码出数据，我们建立了一个文本分类器，从10000文档的语料库中泄露了70%的训练数据，而且对模型精度没有任何负面影响。通过黑盒攻击，我们建立了一个二元性别分类器，可以精确地重建训练集中17个完整的脸部图像，即使这个模型每次查询只泄露了一个比特的信息。

对于黑盒攻击，我们评估了恶意方攻击的成功如何依赖于对训练集的知识。对于在图像上训练的数据，恶意方不需要辅助信息，可以直接使用随机图像作为合成增广输入。对于文本上的训练，我们比较了恶意方知道训练文本的词汇表和恶意方使用从公共语料库汇编的词汇表这两者的精度。

总的来说，使用第三方的代码来训练敏感数据的机器学习模型是危险的，即使代码提供者没有观察训练。我们证明了现代大记忆容量的ML模型是如何泄露数据的，即使在模型是以黑盒子发布而且模型精度和泛化能力没有影响的情况下。

